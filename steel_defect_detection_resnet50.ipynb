{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup Google Colab environment\n",
        "\n",
        "# Mount Google Drive to access project files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install torch torchvision matplotlib pillow scikit-learn\n",
        "\n",
        "# Check directory structure\n",
        "import os\n",
        "\n",
        "# Set the project directory path\n",
        "project_dir = '/content/drive/MyDrive/project'  # Change this to your actual path if needed\n",
        "\n",
        "# Verify the contents of the project directory\n",
        "print(\"Project Directory Contents:\")\n",
        "print(os.listdir(project_dir))\n",
        "\n",
        "# Verify subdirectories (new_images, train, test, model.pth)\n",
        "subdirs = ['new_images', 'train', 'test']\n",
        "for subdir in subdirs:\n",
        "    print(f\"\\nContents of {subdir} folder:\")\n",
        "    subdir_path = os.path.join(project_dir, subdir)\n",
        "    print(os.listdir(subdir_path))\n",
        "\n",
        "# Verify model.pth file exists\n",
        "model_path = os.path.join(project_dir, 'model.pth')\n",
        "print(f\"\\nModel Path: {model_path}\")\n",
        "print(\"Model.pth exists:\", os.path.exists(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6koaCLId_Cn",
        "outputId": "872e8c56-716c-4b99-ea7a-9098d36ffb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Project Directory Contents:\n",
            "['test', 'train', 'model.pth', 'new_images', 'test.txt']\n",
            "\n",
            "Contents of new_images folder:\n",
            "['_1.png', '_2.png', '_13.png', '_3.png', '_4.png', '_5.png', '_6.png', '_7.png', '_8.png', '_9.png', '_10.png', '_11.png', '_12.png', '1.png', '2.png', '6.png', '3.png', '5.png', '7.png', '8.png', '9.png', '10.png', '11.png', '14.png', '15.png', '16.png', '17.png']\n",
            "\n",
            "Contents of train folder:\n",
            "['housing_inclusion_1.png', 'housing_broken.png', 'housing_inclusion_4.png', 'housing_dent_1.png', 'housing_inclusion_3.png', 'housing_tank_iclusion.png', 'housing_sandwash_1.png', 'housing_dent_2.png', 'housing_corelump.png', 'carrier_LNCdamage.png', 'carrier_inclusion_4.png', 'carrier_inclusion_1.png', 'carrier_inclusion_2.png', 'carrier_inclusion_3.png', 'carrier_dent_1.png', 'carrier_morefins.png', 'carrier_dent_3.png', 'carrier_inclusion_5.png', 'carrier_inclusion_6.png', 'carrier_lncdamage_dent.png', 'carrier_inclusion_7.png', 'carrier_inclusion_8.png', 'carrier_dent_2.png', 'carrier_morefins_2.png', 'carrier_dent_4.png', 'carrier_dent_9.png', 'carrier_dent_6.png', 'carrier_inclusion_9.png', 'carrier_inclusion_10.png', 'carrier_letter_missing.png', 'carrier_letter_missing_2.png', 'carrier_dent_8.png', 'carrier_dent_7.png', 'core_inclusion_1.png', 'fist_inclusion.png', 'housing_dent_3.png', 'core_inclusion_2.png', 'core_lump_1.png', 'housing_sandwash_2.png', 'fist_inclusion_1.png', 'housing_inclusion_6.png', 'housing_dent_4.png']\n",
            "\n",
            "Contents of test folder:\n",
            "['housing_inclusion_1.png', 'housing_broken.png', 'housing_inclusion_4.png', 'housing_dent_1.png', 'housing_inclusion_3.png', 'housing_tank_iclusion.png', 'housing_sandwash_1.png', 'housing_dent_2.png', 'housing_corelump.png', 'carrier_LNCdamage.png', 'carrier_inclusion_4.png', 'carrier_inclusion_1.png', 'carrier_inclusion_2.png', 'carrier_inclusion_3.png', 'carrier_dent_1.png', 'carrier_morefins.png', 'carrier_dent_3.png', 'carrier_inclusion_5.png', 'carrier_inclusion_6.png', 'carrier_lncdamage_dent.png', 'carrier_inclusion_7.png', 'carrier_inclusion_8.png', 'carrier_dent_2.png', 'carrier_morefins_2.png', 'carrier_dent_4.png', 'carrier_dent_9.png', 'carrier_dent_6.png', 'carrier_inclusion_9.png', 'carrier_inclusion_10.png', 'carrier_letter_missing.png', 'carrier_letter_missing_2.png', 'carrier_dent_8.png', 'carrier_dent_7.png', 'core_inclusion_1.png', 'fist_inclusion.png', 'housing_dent_3.png', 'core_inclusion_2.png', 'core_lump_1.png', 'housing_sandwash_2.png', 'fist_inclusion_1.png', 'housing_inclusion_6.png', 'housing_dent_4.png']\n",
            "\n",
            "Model Path: /content/drive/MyDrive/project/model.pth\n",
            "Model.pth exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing and Augmentation\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the paths to the train and test directories\n",
        "train_dir = os.path.join(project_dir, 'train')\n",
        "test_dir = os.path.join(project_dir, 'test')\n",
        "\n",
        "# Image Preprocessing and Augmentation\n",
        "# We will define transformations for the images\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),  # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
        "    transforms.RandomRotation(30),     # Random rotation\n",
        "    transforms.ToTensor(),             # Convert image to Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet stats\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),             # Resize for testing\n",
        "    transforms.CenterCrop(224),         # Crop to center 224x224 for consistency\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom Dataset Class to handle loading images and labels\n",
        "class SteelDefectDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Since no labels are provided here, we'll set a dummy label (0) for testing\n",
        "        # Replace this with the actual label if available\n",
        "        label = 0  # Example, replace with actual label if provided\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Create Dataset objects for train and test data\n",
        "train_dataset = SteelDefectDataset(train_dir, transform=train_transforms)\n",
        "test_dataset = SteelDefectDataset(test_dir, transform=test_transforms)\n",
        "\n",
        "# Create DataLoader objects for batching and shuffling\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Checking the first batch of images from the training data\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "print(f\"Batch of images shape: {images.shape}\")\n",
        "print(f\"Batch of labels: {labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTQMvN8GeJU2",
        "outputId": "f094bf68-fd64-4763-f2a1-f3daae8a4099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of images shape: torch.Size([32, 3, 224, 224])\n",
            "Batch of labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Building the Model Architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Define the number of output classes (adjust this according to your dataset)\n",
        "num_classes = 2  # Example, update to your actual number of classes\n",
        "\n",
        "# Load pre-trained ResNet-50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer to match the number of classes\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMiRiUc_el1O",
        "outputId": "4358e8fb-b240-43bb-bd3a-83bb9900b2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Training\n",
        "\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Set hyperparameters\n",
        "num_epochs = 10  # Adjust the number of epochs based on your needs\n",
        "learning_rate = 1e-4  # Fine-tune the learning rate as needed\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Function to evaluate model on the test set\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Average loss for this epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Evaluate model on the test set after every epoch\n",
        "    precision, recall, f1 = evaluate(model, test_loader, device)\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, \"\n",
        "          f\"Precision: {precision*100:.2f}%, Recall: {recall*100:.2f}%, F1 Score: {f1*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86MyAd6en41",
        "outputId": "22cb7cc3-1436-4727-c803-91130481a843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.8630, Precision: 100.00%, Recall: 26.19%, F1 Score: 41.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.6015, Precision: 100.00%, Recall: 64.29%, F1 Score: 78.26%\n",
            "Epoch [3/10], Loss: 0.4023, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [4/10], Loss: 0.2640, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [5/10], Loss: 0.1665, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [6/10], Loss: 0.1111, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [7/10], Loss: 0.0720, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [8/10], Loss: 0.0482, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [9/10], Loss: 0.0350, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
            "Epoch [10/10], Loss: 0.0266, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Save the model state_dict (weights) in a separate file\n",
        "def save_model_state(model, path):\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        # Save model weights\n",
        "        torch.save(model.state_dict(), path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Load the model state_dict (weights)\n",
        "def load_model_state(model, path):\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(path))  # Load only the weights\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        print(f\"Model loaded from {path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "\n",
        "# Define local paths for saving/loading (use a local path first for testing)\n",
        "model_save_path = '/content/model_weights.pth'  # Save it locally first\n",
        "\n",
        "# Save the model after training\n",
        "save_model_state(model, model_save_path)\n",
        "\n",
        "# Initialize the model structure (ResNet50) again and load weights\n",
        "loaded_model = models.resnet50(pretrained=False)  # Initialize a new model structure\n",
        "loaded_model.fc = nn.Linear(loaded_model.fc.in_features, num_classes)  # Adjust the final layer\n",
        "loaded_model = load_model_state(loaded_model, model_save_path)  # Load the saved weights into the new model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On0p1L7Ji5sE",
        "outputId": "b215f167-b5e0-4ec7-c195-e9863536ebef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/model_weights.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/model_weights.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-e2e33d3ed7f4>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))  # Load only the weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import torch\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during inference\n",
        "        for images, labels in test_loader:\n",
        "            # Send images to the device (e.g., GPU if available)\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)  # Get the predicted class with the highest score\n",
        "\n",
        "            # Store predictions and true labels\n",
        "            all_preds.extend(preds.cpu().numpy())  # Convert to numpy for metric calculation\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision * 100:.2f}\")\n",
        "    print(f\"Recall: {recall * 100:.2f}\")\n",
        "    print(f\"F1 Score: {f1 * 100:.2f}\")\n",
        "\n",
        "# Assuming test_loader is already defined\n",
        "evaluate_model(loaded_model, test_loader)  # Evaluate using the loaded model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly4NJ50MjKE7",
        "outputId": "e585f3c0-166e-4436-93d4-5b1d8a75f7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Precision: 100.00\n",
            "Recall: 100.00\n",
            "F1 Score: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the transformation for input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define the model architecture (no pre-trained weights)\n",
        "model = models.resnet50(pretrained=False)  # Initialize ResNet-50 without pre-trained weights\n",
        "num_classes = 2  # Adjust based on your dataset\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)  # Modify final layer\n",
        "\n",
        "# If you don't have the model weights saved, you can skip the loading step and initialize weights randomly\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/project/model.pth'))  # This line is now not needed\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the new images directory\n",
        "new_images_dir = '/content/drive/MyDrive/project/new_images'\n",
        "\n",
        "# Load new images from the directory\n",
        "image_paths = [os.path.join(new_images_dir, img) for img in os.listdir(new_images_dir) if img.endswith(('.png', '.jpg'))]\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "true_labels = []\n",
        "predictions = []\n",
        "\n",
        "# Perform batch inference on the images\n",
        "for image_path in image_paths:\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')  # Convert image to RGB if it's in another mode\n",
        "    image = transform(image).unsqueeze(0)  # Apply transformations and add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted_label = torch.max(outputs, 1)  # Get the predicted class\n",
        "        predictions.append(predicted_label.item())\n",
        "\n",
        "# Example for evaluating accuracy and confusion matrix (you need ground truth labels for this)\n",
        "# Assume true_labels are manually set based on your dataset (or from directory names)\n",
        "# Here I'm just assuming class '0' for all images for illustration purposes\n",
        "true_labels = [0] * len(predictions)  # Replace with your actual labels for new images\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "precision = precision_score(true_labels, predictions, average='weighted', zero_division=1)\n",
        "recall = recall_score(true_labels, predictions, average='weighted', zero_division=1)\n",
        "f1 = f1_score(true_labels, predictions, average='weighted', zero_division=1)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "iQnNy-sktRVI",
        "outputId": "502686ed-8593-4e64-f75c-2aa26e8956e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}